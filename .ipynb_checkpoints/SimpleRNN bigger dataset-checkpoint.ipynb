{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.io import wavfile \n",
    "from librosa.feature import mfcc\n",
    "from librosa import load\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Activation, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "INPUT_SIZE = 20\n",
    "BATCH_SIZE = 100\n",
    "BATCH_INDEX = 0\n",
    "OUTPUT_SIZE = 10\n",
    "CELL_SIZE = 50\n",
    "LR = 5e-4\n",
    "\n",
    "PATH = 'SpeechCommands/speech_commands_v0.02'\n",
    "DIGITS = '0123456789'\n",
    "LABELS = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_files():\n",
    "    D_train, D_test = [], []\n",
    "    with open('trainingspeakers.txt') as fileobj:\n",
    "        trainingspeakers = [line.strip() for line in fileobj]\n",
    "    fileobj.close()\n",
    "    with open('validationspeakers.txt') as fileobj:\n",
    "        validationspeakers = [line.strip() for line in fileobj]\n",
    "    fileobj.close()\n",
    "    for d in DIGITS:\n",
    "        dpath = os.path.join(PATH, LABELS[int(d)])\n",
    "        for filename in [x for x in os.listdir(dpath)]:\n",
    "            filepath = os.path.join(dpath, filename)\n",
    "            sig, rate = load(filepath)\n",
    "\n",
    "            # Extract MFCC features\n",
    "            mfcc_features = mfcc(sig, rate).T\n",
    "            dp = np.vstack(([int(d)]*INPUT_SIZE, mfcc_features))\n",
    "            if filename.split('_')[0] in validationspeakers:\n",
    "                D_test.append(dp)\n",
    "            else:\n",
    "                D_train.append(dp)\n",
    "    lt = len(D_train)\n",
    "    D_train.extend(D_test)\n",
    "    D = tf.keras.preprocessing.sequence.pad_sequences(D_train, padding=\"post\")\n",
    "    D_train = D[:lt]\n",
    "    D_test = D[lt:]\n",
    "    np.random.shuffle(D_train)\n",
    "    np.random.shuffle(D_test)\n",
    "    X_train = [dp[1:] for dp in D_train]\n",
    "    X_test = [dp[1:] for dp in D_test]\n",
    "    y_train = [dp[0][0] for dp in D_train]\n",
    "    y_test = [dp[0][0] for dp in D_test]\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
    "    y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
    "    return (np.array(X_train), y_train), (np.array(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traind, testd = load_wav_files()\n",
    "X_train, y_train = traind\n",
    "X_test, y_test = testd\n",
    "scl = np.max(np.abs(X_train))\n",
    "X_train = X_train/scl\n",
    "X_test = X_test/scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# RNN cell\n",
    "model.add(SimpleRNN(\n",
    "    batch_input_shape=(None, TIME_STEPS, INPUT_SIZE),\n",
    "    units=CELL_SIZE,\n",
    "    activation=\"elu\",\n",
    "    kernel_initializer=\"orthogonal\",\n",
    "    recurrent_regularizer=\"l2\"\n",
    "))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# optimizer\n",
    "adam = Adam(tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    LR,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.94,\n",
    "    staircase=True))\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  2.8095529079437256 train accuracy:  0.10377146303653717\n",
      "test loss:  2.809520721435547 test accuracy:  0.10563652962446213\n",
      "train loss:  1.2783865928649902 train accuracy:  0.6426576972007751\n",
      "test loss:  1.2976655960083008 test accuracy:  0.6434928178787231\n",
      "train loss:  1.1305230855941772 train accuracy:  0.70171719789505\n",
      "test loss:  1.1505022048950195 test accuracy:  0.6996001601219177\n",
      "train loss:  1.0664575099945068 train accuracy:  0.7273952960968018\n",
      "test loss:  1.092832326889038 test accuracy:  0.7228170037269592\n",
      "train loss:  1.0390204191207886 train accuracy:  0.7394960522651672\n",
      "test loss:  1.0655114650726318 test accuracy:  0.7351992726325989\n",
      "train loss:  1.0107507705688477 train accuracy:  0.7502808570861816\n",
      "test loss:  1.0390881299972534 test accuracy:  0.7444860339164734\n",
      "train loss:  1.0001981258392334 train accuracy:  0.7544214129447937\n",
      "test loss:  1.028967022895813 test accuracy:  0.7469366788864136\n",
      "train loss:  0.9940862655639648 train accuracy:  0.7566682696342468\n",
      "test loss:  1.023108720779419 test accuracy:  0.7501612305641174\n",
      "train loss:  0.991033673286438 train accuracy:  0.7575027942657471\n",
      "test loss:  1.0204684734344482 test accuracy:  0.7502902150154114\n",
      "train loss:  0.9896185398101807 train accuracy:  0.7580805420875549\n",
      "test loss:  1.0193169116973877 test accuracy:  0.7506771683692932\n",
      "train loss:  0.9888364672660828 train accuracy:  0.7579842805862427\n",
      "test loss:  1.0186822414398193 test accuracy:  0.7505481839179993\n",
      "train loss:  0.9884206056594849 train accuracy:  0.7579200863838196\n",
      "test loss:  1.0183836221694946 test accuracy:  0.7509351372718811\n",
      "train loss:  0.9882032871246338 train accuracy:  0.7579521536827087\n",
      "test loss:  1.018189787864685 test accuracy:  0.7508061528205872\n",
      "train loss:  0.9880884885787964 train accuracy:  0.7580805420875549\n",
      "test loss:  1.018073558807373 test accuracy:  0.751193106174469\n",
      "train loss:  0.9880274534225464 train accuracy:  0.7580163478851318\n",
      "test loss:  1.0180028676986694 test accuracy:  0.751193106174469\n",
      "train loss:  0.9879952669143677 train accuracy:  0.7580484747886658\n",
      "test loss:  1.0179646015167236 test accuracy:  0.751193106174469\n",
      "train loss:  0.9879796504974365 train accuracy:  0.7580163478851318\n",
      "test loss:  1.017948031425476 test accuracy:  0.751193106174469\n",
      "train loss:  0.9879727363586426 train accuracy:  0.7580163478851318\n",
      "test loss:  1.017944574356079 test accuracy:  0.751193106174469\n",
      "train loss:  0.9879701733589172 train accuracy:  0.7580163478851318\n",
      "test loss:  1.0179438591003418 test accuracy:  0.751193106174469\n",
      "train loss:  0.9879693984985352 train accuracy:  0.7580163478851318\n",
      "test loss:  1.0179433822631836 test accuracy:  0.7513220906257629\n",
      "train loss:  0.9879691004753113 train accuracy:  0.7580163478851318\n",
      "test loss:  1.0179426670074463 test accuracy:  0.7513220906257629\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for step in range(100001):\n",
    "    # data shape = (batch_num, steps, inputs/outputs)\n",
    "    X_batch = X_train[BATCH_INDEX: BATCH_INDEX+BATCH_SIZE, :, :]\n",
    "    Y_batch = y_train[BATCH_INDEX: BATCH_INDEX+BATCH_SIZE, :]\n",
    "    cost = model.train_on_batch(X_batch, Y_batch)\n",
    "    BATCH_INDEX += BATCH_SIZE\n",
    "    BATCH_INDEX = 0 if BATCH_INDEX >= X_train.shape[0] else BATCH_INDEX\n",
    "\n",
    "    if step % 5000 == 0:\n",
    "        loss, accuracy = model.evaluate(X_train, y_train, batch_size=y_train.shape[0], verbose=False)\n",
    "        print('train loss: ', loss, 'train accuracy: ', accuracy)\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, batch_size=y_test.shape[0], verbose=False)\n",
    "        print('test loss: ', loss, 'test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=-1)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[685,   0,  70,  28,  15,   0,   5,  22,   1,   6],\n",
       "       [  5, 546,   0,   5,  36,  70,   0,   9,   3,  93],\n",
       "       [ 60,   2, 518,  95,   5,   1,  11,  51,  13,   8],\n",
       "       [  3,   0,  64, 518,   0,   9,   1,  41,  37,  62],\n",
       "       [ 16,  37,   4,   7, 613,  17,   2,  44,   4,  12],\n",
       "       [  3, 119,   0,   5,  14, 516,   1,   9,  16, 103],\n",
       "       [  3,   0,  36,  21,   1,   4, 651,  25,  35,   3],\n",
       "       [  4,   0,  41,  32,  31,   9,   6, 637,   5,  26],\n",
       "       [  2,   3,  10,  89,   1,  24,  13,   6, 592,  13],\n",
       "       [  0, 111,   2,  38,   9,  55,   0,  24,   2, 549]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
